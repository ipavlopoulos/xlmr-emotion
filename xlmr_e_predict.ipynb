{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi6zjpKZz1+o4tDvBsvlfc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipavlopoulos/xlmr-emotion/blob/main/xlmr_e_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "ZcdfJj5MIwbd"
      },
      "outputs": [],
      "source": [
        "#@title Source Code\n",
        "%%capture\n",
        "!pip install transformers[sentencepiece]\n",
        "\n",
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input,Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "from tensorflow import keras\n",
        "#from google_trans_new import google_translator\n",
        "\n",
        "# Define the model architecture (same as before)\n",
        "def build_model(transformer, max_len):\n",
        "    np.random.seed(2909)\n",
        "    set_seed(1995)\n",
        "\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "\n",
        "    # Define the Lambda layer with output_shape\n",
        "    def transformer_wrapper(inputs):\n",
        "        return transformer(inputs)[0]\n",
        "\n",
        "    # Get the shape of the output tensor from the transformer model\n",
        "    output_shape = (max_len, transformer.config.hidden_size)  # (sequence_length, hidden_size)\n",
        "\n",
        "    sequence_output = Lambda(transformer_wrapper, output_shape=output_shape)(input_word_ids)\n",
        "    cls_token = sequence_output[:, 0, :]  # Extract the [CLS] token representation\n",
        "    out = Dense(8, activation='sigmoid')(cls_token)  # Output layer\n",
        "\n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Model configuration\n",
        "model_name = \"jplu/tf-xlm-roberta-base\"\n",
        "\n",
        "# Instantiate the transformer model\n",
        "transformer_layer = TFAutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the weights\n",
        "# downloading the weights\n",
        "%%capture\n",
        "!wget https://github.com/ipavlopoulos/xlmr-emotion/raw/main/xlmr-e.weights.h5"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WQsqPAFrWuCQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the model\n",
        "\n",
        "# Build the new model\n",
        "max_len=109\n",
        "new_model = build_model(transformer_layer, max_len=max_len)\n",
        "\n",
        "# Load the saved weights\n",
        "new_model.load_weights('xlmr-e.weights.h5')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B1sLGxRTNaXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Predict for a new text\n",
        "\n",
        "#Function for encoding according to XLM-R\n",
        "def encode(texts, tokenizer, maxlen):\n",
        "    dic = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        return_attention_mask=False, #need only the input_ids\n",
        "        truncation=True,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    return np.array(dic['input_ids'])\n",
        "\n",
        "emotions = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"trust\"]\n",
        "maxlen = 109\n",
        "def predict(texts, model=new_model, tokenizer=tokenizer, maxlen=109, emotion=emotions):\n",
        "    encoded_texts = encode(texts, tokenizer, maxlen)\n",
        "    predictions = model.predict(encoded_texts)\n",
        "    return [dict(zip(emotions+[\"none\"], list(p)+[1-max(p)])) for p in predictions]\n",
        "\n",
        "predict(texts=[\"this is a text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "oABv8-JsJQ5x",
        "outputId": "5bf01995-49a8-4e39-fbae-d849e75cf7fd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'anger': 0.3009773,\n",
              "  'anticipation': 0.11039856,\n",
              "  'disgust': 0.33634093,\n",
              "  'fear': 0.14897384,\n",
              "  'joy': 0.3111991,\n",
              "  'sadness': 0.2342824,\n",
              "  'surprise': 0.044223376,\n",
              "  'trust': 0.039620537,\n",
              "  'none': 0.6636590659618378}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ue5ZyT6vP0f_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}